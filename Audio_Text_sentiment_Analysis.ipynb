{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnalihinukale/swapnalihinukale/blob/main/Audio_Text_sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8MfT_UtpJABK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d438e24-becc-47b9-b60c-eea478f02838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keyBERT\n",
            "  Downloading keybert-0.7.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentence-transformers>=0.3.8\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.8/dist-packages (from keyBERT) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from keyBERT) (1.22.4)\n",
            "Collecting rich>=10.4.0\n",
            "  Downloading rich-13.3.1-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.14.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.1.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.4.0->keyBERT) (4.5.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keyBERT) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keyBERT) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keyBERT) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keyBERT) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keyBERT) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keyBERT) (0.14.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keyBERT) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keyBERT) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keyBERT) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keyBERT) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keyBERT) (2.25.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keyBERT) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers>=0.3.8->keyBERT) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers>=0.3.8->keyBERT) (8.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keyBERT) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keyBERT) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keyBERT) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keyBERT) (1.24.3)\n",
            "Building wheels for collected packages: keyBERT, sentence-transformers\n",
            "  Building wheel for keyBERT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keyBERT: filename=keybert-0.7.0-py3-none-any.whl size=23800 sha256=085ed670a50a732b0f9dab411f363de0e7a935b50ab9af7f4894dfbe8aa5097c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/bc/8b/a51bee77aec33895e6c8c236144b4cc10875659c4d2c80f070\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=3ec6421c5731666a3e5b6950b05f59289910035d1c17d97f4c052fd8301ec897\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built keyBERT sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, pygments, mdurl, markdown-it-py, huggingface-hub, transformers, rich, sentence-transformers, keyBERT\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.12.1 keyBERT-0.7.0 markdown-it-py-2.2.0 mdurl-0.1.2 pygments-2.14.0 rich-13.3.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "!pip install keyBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wfqULx2VB4iB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f9e201-0886-4208-f75f-9d300dd7f493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AURirAwzh4bG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997dd341-9c16-4c7a-f6d7-0f569aef8a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nrclex\n",
            "  Downloading NRCLex-4.0-py3-none-any.whl (4.4 kB)\n",
            "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.8/dist-packages (from nrclex) (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.8/dist-packages (from textblob->nrclex) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob->nrclex) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob->nrclex) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob->nrclex) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob->nrclex) (1.2.0)\n",
            "Building wheels for collected packages: nrclex\n",
            "  Building wheel for nrclex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nrclex: filename=NRCLex-3.0.0-py3-none-any.whl size=43329 sha256=de49cf60ec18111e91bdb8bef5b4d7440fb8899b385975eb216256e6efddff3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/95/c0/42b43fb15eb48e4f5a67cba8915540cb2783591c59c037a9e5\n",
            "Successfully built nrclex\n",
            "Installing collected packages: nrclex\n",
            "Successfully installed nrclex-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nrclex"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!pip install pip install SpeechRecognition"
      ],
      "metadata": {
        "id": "Rth5R_TNWgTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8wpPJ0PA5Ue"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pydub import AudioSegment\n",
        "import speech_recognition as sr\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import email\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "from collections import Counter\n",
        "\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from textblob import TextBlob\n",
        "\n",
        "import nltk.data\n",
        "\n",
        "\n",
        "#nltk.download('punkt')\n",
        "from nltk.collocations import *\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stop_words')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from keybert import KeyBERT\n",
        "\n",
        "from nrclex import NRCLex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spliting Audio file into chunks (20s)**"
      ],
      "metadata": {
        "id": "mDaG5qIhb5_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input audio file to be sliced\n",
        "audio = AudioSegment.from_wav(\"/content/drive/MyDrive/audio_text_sentiment/audio.wav\")\n",
        "n = len(audio)\n",
        "\n",
        "# Variable to count the number of sliced chunks\n",
        "counter = 1\n",
        "\n",
        "# Interval length at which to slice the audio file.\n",
        "# If length is 86 seconds, and interval is 20 seconds,\n",
        "# The chunks created will be:\n",
        "# chunk1 : 0 - 20 seconds\n",
        "# chunk2 : 20 - 40 seconds\n",
        "# chunk3 : 40 - 60 seconds\n",
        "\n",
        "interval = 20 * 1000\n",
        "time=[]\n",
        "\n",
        "overlap = 0.0 * 1000\n",
        "\n",
        "# Initialize start and end seconds to 0\n",
        "start = 0\n",
        "end = 0\n",
        "\n",
        "# Flag to keep track of end of file.\n",
        "# When audio reaches its end, flag is set to 1 and we break\n",
        "flag = 0\n",
        "\n",
        "# Iterate from 0 to end of the file,\n",
        "# with increment = interval\n",
        "for i in range(0, n, interval):\n",
        "    # During first iteration,\n",
        "    # start is 0, end is the interval\n",
        "    if i == 0:\n",
        "        start = 0\n",
        "        end = interval\n",
        "    \n",
        "    # All other iterations,\n",
        "    # start is the previous end - overlap\n",
        "    # end becomes end + interval\n",
        "    else:\n",
        "        start = end - overlap\n",
        "        end = start + interval\n",
        "        \n",
        "    # When end becomes greater than the file length,\n",
        "    # end is set to the file length\n",
        "    # flag is set to 1 to indicate break.\n",
        "    if end >= n:\n",
        "        end = n\n",
        "        flag = 1\n",
        "        \n",
        "    # Storing audio file from the defined start to end \n",
        "    chunk = audio[start:end]\n",
        "    # Filename / Path to store the sliced audio\n",
        "    \n",
        "    filename = '/content/drive/MyDrive/audio_text_sentiment/chunks/chunk'+str(counter)+'.wav'\n",
        "    \n",
        "    # Store the sliced audio file to the defined path\n",
        "    chunk.export(filename, format =\"wav\")\n",
        "    # Print information about the current chunk\n",
        "    print(\"Processing chunk \"+str(counter)+\". Start = \"+str(start)+\" end = \"+str(end))\n",
        "    \n",
        "    a = str(start/1000)+ \"-\"+str(end/1000)\n",
        "    time.append(a)\n",
        "    # Increment counter for the next chunk\n",
        "    counter = counter + 1\n",
        "    # Slicing of the audio file is done.\n",
        "    # Skip the below steps if there is some other usage\n",
        "    # for the sliced audio files.\n",
        "    \n",
        "\n",
        "AUDIO_FILE = filename\n",
        "\n",
        "# Initialize the recognizer\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# Traverse the audio file and listen to the audio\n",
        "with sr.AudioFile(AUDIO_FILE) as source:\n",
        "    audio_listened = r.listen(source)\n",
        "\n",
        "# Try to recognize the listened audio\n",
        "# And catch expections.\n",
        "try:\n",
        "    rec = r.recognize_google(audio_listened)\n",
        "    \n",
        "# If recognized, write into the file.\n",
        "# fh.write(rec+\" \")\n",
        "# If google could not understand the audio\n",
        "except sr.UnknownValueError:\n",
        "    print(\"Could not understand audio\")\n",
        "    \n",
        "# If the results cannot be requested from Google.\n",
        "# Probably an internet connection error.\n",
        "except sr.RequestError as e:\n",
        "    print(\"Could not request results.\")\n",
        "    \n",
        "# Check for flag.\n",
        "# If flag is 1, end of the whole audio reached.\n",
        "# Close the file and break.\n",
        "# if flag == 1:\n",
        "#     break\n"
      ],
      "metadata": {
        "id": "6F1dEfoHWuHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Audio To Text Function**\n"
      ],
      "metadata": {
        "id": "3RvRe7KvW6Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def audio_to_text(filename):\n",
        "    '''\n",
        "        Converts speech to text\n",
        "    '''\n",
        "    # Get recognizer\n",
        "    r = sr.Recognizer() \n",
        "    \n",
        "    with sr.AudioFile(filename) as source:\n",
        "        audio_listened = r.record(source) \n",
        "\n",
        "        # Try to recognize the listened audio \n",
        "        # And catch expections. \n",
        "        try:     \n",
        "            return r.recognize_google(audio_listened, show_all = False) \n",
        "            \n",
        "\n",
        "        # If google could not understand the audio \n",
        "        except sr.UnknownValueError: \n",
        "            print(\"Could not understand audio provided\") \n",
        "            return None\n",
        "\n",
        "        # If the results cannot be requested from Google. \n",
        "        # Probably an internet connection error. \n",
        "        except sr.RequestError as e: \n",
        "            print(\"Could not request results.\") \n",
        "            return None"
      ],
      "metadata": {
        "id": "PGyy2vWCWyPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chunks to Text Conversion**"
      ],
      "metadata": {
        "id": "dlVF2IJsXehl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunkAudioToText(): \n",
        "    '''\n",
        "    gives text output of chunks\n",
        "    '''\n",
        "    \n",
        "    i = 1\n",
        "    answer=[]\n",
        "    for chunk in os.listdir('/content/drive/MyDrive/audio_text_sentiment/chunks'): \n",
        "        file = '/content/drive/MyDrive/audio_text_sentiment/chunks/chunk'+str(i)+'.wav'\n",
        "        print(\"Processing chunk \"+str(i)) \n",
        "        rec = audio_to_text(file) # function which  do Speech to text conversion(IBM Watson SpeechToText API)\n",
        "        answer.append(rec)\n",
        "        if rec == \"Error5487\":\n",
        "            return \"Error5487E\"\n",
        "        \n",
        "        i += 1\n",
        "    \n",
        "    \n",
        "    return answer"
      ],
      "metadata": {
        "id": "D5Qz_FlEW_9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating DataFrame**"
      ],
      "metadata": {
        "id": "hTfv1behXqqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = {\"Timestamp\": time, 'Answer':chunkAudioToText()}\n",
        "df = pd.DataFrame.from_dict(d1)\n",
        "df"
      ],
      "metadata": {
        "id": "6PrdVRFlXH24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/Audio_Data.csv\", index=False)"
      ],
      "metadata": {
        "id": "7WExqVPpXOOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPWorpjxMfEt"
      },
      "source": [
        "# **cleaned Text**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jmE-mi-nBj76"
      },
      "outputs": [],
      "source": [
        " def readExcelFile(input_file_path):\n",
        "    df = pd.read_csv(input_file_path)\n",
        "    return df\n",
        "\n",
        "   \n",
        " def clean_column(data):\n",
        "    if data is not None:\n",
        "\n",
        "        data=str(data) # convert to string data type\n",
        "        data = data.lower()#lower casing\n",
        "        #replcae with space\n",
        "        data = re.sub('-', '', data)\n",
        "        data = re.sub('_', '', data)\n",
        "        # Remove data between square brackets\n",
        "        data =re.sub('\\[[^]]*\\]', '', data)\n",
        "        # removes punctuation\n",
        "        data = re.sub(r'[^\\w\\s]','',data)\n",
        "        data = re.sub(r'\\n',' ',data)\n",
        "        data = re.sub(r'[0-9]+','',data)\n",
        "        data = re.sub(r\"[.?]+\",'. ' ,data) \n",
        "        # removal of urls\n",
        "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        data=url_pattern.sub(r'',data)\n",
        "        #removal html tags\n",
        "        html_pattern = re.compile('<.*?>')\n",
        "        data = html_pattern.sub('', data)\n",
        "        data = re.sub(r'\\n','' ,data) #remove newline\n",
        "        data = re.sub(r'\\r','' ,data)\n",
        "        data = re.sub(r'/c', '' ,data)\n",
        "        data = re.sub(r\"[^\\w'.,?\\s]+\",' ' ,data) #remove all except words, req punctuation marks and space\n",
        "        data = data.encode('utf-8').decode('ascii','ignore') #remove non-ascii characters\n",
        "        data = re.sub(r'\\s+',' ' ,data) #remove extra space b/w words\n",
        "        #chat word conversion\n",
        "        data = re.sub(r\"\\'ve\", \" have \", data)\n",
        "        data = re.sub(r\"can't\", \"cannot \", data)\n",
        "        data = re.sub(r\"n't\", \" not \", data)\n",
        "        data = re.sub(r\"I'm\", \"I am\", data)\n",
        "        data = re.sub(r\" m \", \" am \", data)\n",
        "        data = re.sub(r\"\\'re\", \" are \", data)\n",
        "        data = re.sub(r\"\\'d\", \" would \", data)\n",
        "        data = re.sub(r\"\\'ll\", \" will \", data)\n",
        "        \n",
        "        return data\n",
        "        \n",
        "        \n",
        "def CleanedReviewTextT():\n",
        "  df=readExcelFile('/content/drive/MyDrive/Audio_Data.csv') # csv file\n",
        " \n",
        "\n",
        "  #split the whole review on punctuation(.) in single sentense\n",
        "  s = df[\"Answer\"].str.split('.',expand = True).apply(pd.Series,1).stack()\n",
        "  s.index = s.index.droplevel(-1)\n",
        "  s.name = 'Answer'\n",
        "  x = s.to_frame(name='new_review')\n",
        "  df = df.join(x)\n",
        "  #apply clean column\n",
        "  df['new_review'] = df['new_review'].apply(clean_column)\n",
        "  #remove stopwords\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  df['Answer']=str(df['Answer'])\n",
        "\n",
        "  df['CleanedReviewText'] = df['new_review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "  \n",
        "\n",
        "  return df    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPKR5vljH_C0"
      },
      "source": [
        "# **Sentiments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4giHNZgENwu"
      },
      "outputs": [],
      "source": [
        "def sentiment_scores(sentence): \n",
        "  \n",
        "    # Create a SentimentIntensityAnalyzer object.\n",
        "    sid_obj = TextBlob(sentence).sentiment[0]\n",
        "  \n",
        "    return sid_obj\n",
        "\n",
        "def Sentiment_column():\n",
        "  df=CleanedReviewTextT()\n",
        "  \n",
        "  df.drop(df.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
        "\n",
        "  df['CleanedReviewText'] = df['CleanedReviewText'].astype(str)\n",
        "  df['Sentiment_score_subsent'] = df['CleanedReviewText'].apply(sentiment_scores)\n",
        "  df['Sentiment_per_subsentence'] = [\"positive\" if x>0 else(\"Negative\" if x<0 else \"Neutral\") for x in df[\"Sentiment_score_subsent\"] ]\n",
        "  df['ReviewLength'] = df['CleanedReviewText'].apply(len)\n",
        "  df['sentiment_substance_review_length'] = df['Sentiment_score_subsent']*df['ReviewLength']\n",
        "  df['polarity_sign'] = np.sign(df.Sentiment_score_subsent)\n",
        "  df.drop('Answer',axis=1,inplace=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03Nu-t0uq6c1"
      },
      "outputs": [],
      "source": [
        "Sentiment_column()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxQ0kNJMMFNZ"
      },
      "source": [
        "# **Keyword_list**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0NLygrmIdJk"
      },
      "outputs": [],
      "source": [
        "def Keyword_list():\n",
        "  df=Sentiment_column()\n",
        "  df= df.explode('CleanedReviewText').reset_index()\n",
        "  df['keywords_list'] = pd.Series()\n",
        "  df['keywords_score'] = pd.Series()\n",
        "\n",
        "\n",
        "  kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
        "  for i in range(0,len(df.CleanedReviewText)):\n",
        "      \n",
        "      doc = str(df['CleanedReviewText'][i])\n",
        "    \n",
        "      stop_words = \"english\"\n",
        "\n",
        "      keyword = kw_model.extract_keywords(doc,\n",
        "\n",
        "            keyphrase_ngram_range=(0,1),\n",
        "\n",
        "            stop_words='english',\n",
        "\n",
        "            highlight=False,\n",
        "\n",
        "            top_n=3)\n",
        "\n",
        "     \n",
        "      df['keywords_list'][i]= np.array(list(dict(keyword).keys()),dtype=object)\n",
        "      df['keywords_score'][i]=np.array(list(dict(keyword).values()),dtype=object)\n",
        "\n",
        "  return df\n",
        "\n",
        "     \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAvg8DPpFGPd"
      },
      "outputs": [],
      "source": [
        "Keyword_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKwjeHETMJKV"
      },
      "source": [
        "# **Bigram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFX1p4hJDvyC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOoVqKj0JX8v"
      },
      "outputs": [],
      "source": [
        "def bigram():\n",
        "  df =Keyword_list()\n",
        "  df['bigram_list'] = pd.Series()\n",
        "  df['bigram_score'] = pd.Series()\n",
        "\n",
        "\n",
        "  kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
        "  for i in range(len(df.CleanedReviewText)):\n",
        "      # print(i)\n",
        "      doc = str(df['CleanedReviewText'][i])\n",
        "      # print(doc)\n",
        "      stop_words = \"english\"\n",
        "\n",
        "      keyword = kw_model.extract_keywords(doc,\n",
        "\n",
        "            keyphrase_ngram_range=(0,2),\n",
        "\n",
        "            stop_words='english',\n",
        "\n",
        "            highlight=False,\n",
        "\n",
        "            top_n=3)\n",
        "\n",
        "     \n",
        "      df['bigram_list'][i]= np.array(list(dict(keyword).keys()),dtype=object)\n",
        "      df['bigram_score'][i]=np.array(list(dict(keyword).values()),dtype=object)\n",
        "\n",
        "  return df    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYbXzLz3MM1_"
      },
      "source": [
        "# **Trigram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6PXd6uqJjjF"
      },
      "outputs": [],
      "source": [
        "def trigram():\n",
        "  df =bigram() \n",
        "  df['trigram_list'] = pd.Series()\n",
        "  df['trigram_score'] = pd.Series()\n",
        "\n",
        "\n",
        "  kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
        "  for i in range(len(df.CleanedReviewText)):\n",
        "      # print(i)\n",
        "      doc = str(df['CleanedReviewText'][i])\n",
        "      # print(doc)\n",
        "      stop_words = \"english\"\n",
        "\n",
        "      keyword = kw_model.extract_keywords(doc,\n",
        "\n",
        "            keyphrase_ngram_range=(0,3),\n",
        "\n",
        "            stop_words='english',\n",
        "\n",
        "            highlight=False,\n",
        "\n",
        "            top_n=3)\n",
        "\n",
        "     \n",
        "      df['trigram_list'][i]= np.array(list(dict(keyword).keys()),dtype=object)\n",
        "      df['trigram_score'][i]=np.array(list(dict(keyword).values()),dtype=object)\n",
        "\n",
        "  return df   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFiI-Zl1NMqm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def emotion(x):\n",
        "    text = NRCLex(x)\n",
        "    if text.top_emotions[0][1] == 0.0:\n",
        "        return \"No emotion\"\n",
        "    else:\n",
        "        return text.top_emotions[0][0]\n",
        "    \n",
        "def emotion_column():\n",
        "  df=trigram()\n",
        "  df['Emotion'] = df['CleanedReviewText'].apply(emotion)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MnnzeneN7fG"
      },
      "source": [
        "# **EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65uqCm-HN-Tc"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "df=Sentiment_column()\n",
        "sns.countplot(df['Sentiment_per_subsentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNAAxJkgOPyU"
      },
      "outputs": [],
      "source": [
        "df=emotion_column()\n",
        "df.drop('index',inplace=True,axis=1)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9R8qACor-Yo"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/audio_text_sentiment/audio_output.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HaxLNPADbFFi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNumqJAq5xq/X5w3IRpvFI3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}